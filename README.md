# üß† Chat Console com Semantic Kernel + Plugins + Ollama

Este projeto √© um **chat console em .NET** integrado ao **Semantic Kernel** e ao provedor local **Ollama**, permitindo conversas com modelos como **Llama 3.1** diretamente no terminal.  
Al√©m disso, o projeto utiliza **Plugins de IA**, que permitem ao modelo executar fun√ß√µes reais implementadas em C#.

---

## üöÄ Tecnologias Utilizadas

- **.NET 9**
- **C#**
- **Semantic Kernel (Microsoft.SemanticKernel)**
- **Connectors Ollama**
- **Microsoft.Extensions.Logging**
- **Microsoft.Extensions.DependencyInjection**
- **Ollama (modelo local: llama3.1:latest)**

---

## üí¨ Funcionamento do Chat

O programa cria um **loop de conversa**, onde:

- o usu√°rio envia uma mensagem pelo console  
- a mensagem √© adicionada ao `ChatHistory`  
- o Semantic Kernel envia o hist√≥rico e o plugin para o modelo  
- o Ollama gera a resposta  
- o hist√≥rico √© atualizado novamente

Isso cria uma experi√™ncia de chat **contextual**, onde o modelo entende toda a conversa anterior.

---

## üîÑ Uso de Hist√≥rico (ChatHistory)

O ChatHistory guarda:

- todas as mensagens do usu√°rio  
- respostas do assistente  
- contexto passado ao modelo  
- chamadas internas do plugin

O modelo sempre recebe o hist√≥rico completo, permitindo respostas mais naturais e coerentes.

---

## üîå Plugins do Semantic Kernel

O projeto possui um **plugin chamado ProductPlugin, que exp√µe fun√ß√µes reais para o LLM:

### Fun√ß√µes inclu√≠das:
- **get_product** ‚Üí retorna todos os produtos  
- **get_state** ‚Üí altera os dados de um produto espec√≠fico  

Essas fun√ß√µes s√£o marcadas com:
csharp
[KernelFunction]
Isso permite que o modelo:

descubra automaticamente quais fun√ß√µes existem
interprete par√¢metros

execute a√ß√µes reais dentro da aplica√ß√£o

retorne resultados processados

Tudo isso usando Function Calling Autom√°tico.

üß© Integra√ß√£o com Semantic Kernel

O Semantic Kernel √© respons√°vel por:

gerenciar servi√ßos via DI
registrar plugins
coordenar chamadas do modelo
organizar o ciclo de execu√ß√£o do chat
unificar a experi√™ncia com IA no .NET

Ele funciona como o ‚Äúorquestrador‚Äù da sua aplica√ß√£o GenAI.

ü§ñ Integra√ß√£o com Ollama
O conector:

Microsoft.SemanticKernel.Connectors.Ollama

permite que o Semantic Kernel converse com modelos locais do Ollama.
Vantagens:

funciona offline
r√°pido e leve
custo zero
ideal para testes e desenvolvimento local

Modelo utilizado:
llama3.1:latest

‚úÖ Finalidade do Projeto
Este projeto demonstra:

como integrar Semantic Kernel com Ollama
como criar um chat local com hist√≥rico
como expor fun√ß√µes reais (plugins) para serem chamadas por uma LLM
como usar Function Calling Autom√°tico
como estruturar um agente simples em .NET

√ìtimo para estudos, portf√≥lio e para entender a base da GenAI com .NET.
